{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree"
      ],
      "metadata": {
        "id": "Fk6ARiIb4Duk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q.1-** What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        "\n",
        " A Decision Tree is a machine learning model that is used for making predictions by following a step-by-step questioning process. It is structured like a tree, where each internal node represents a question or test on a feature, each branch represents the outcome of that question, and each leaf node represents the final decision or prediction.\n",
        "\n",
        " How it works in classification:\n",
        "1.\tStart at the root: The algorithm looks at all features (like age, income, gender, etc.).\n",
        "2.\tChoose the best feature: It picks the feature that best separates the data into classes (using measures like Gini index or Entropy/Information Gain).\n",
        "3.\tSplit the data: It divides the dataset into branches based on that feature.\n",
        "4.\tRepeat: For each branch, it again finds the best feature and splits further.\n",
        "5.\tStop: This continues until:\n",
        "o\tAll data in a branch belongs to one class, OR\n",
        "o\tThe tree reaches a stopping condition (like max depth).\n",
        "6.\tPrediction: For a new input, the model follows the path of decisions down the tree until it reaches a leaf, and then assigns the class.\n",
        "\n",
        "\n",
        "**Q2.** Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?\n",
        "\n",
        "- Gini Impurity:\n",
        "\n",
        " 1. It tells us how often we would be wrong if we randomly picked a class label in a node.\n",
        " 2. Value is 0 when all data in that node belongs to one class (perfectly pure).\n",
        " 3.\tThe more mixed the classes are, the higher the Gini value.\n",
        "\n",
        "- Entropy\n",
        " 4. It tells us how much disorder or uncertainty is in a node.\n",
        " 5. Value is 0 when the node is pure (all same class).\n",
        " 6. The value is higher when classes are more evenly mixed (maximum confusion).\n",
        "\n",
        "- How they affect splits in a Decision Tree:\n",
        "\n",
        " 1. When building the tree, the algorithm checks different features and decides where to split the data.\n",
        " 2. It chooses the split that makes the child nodes purer (closer to one class only).\n",
        " 3. Gini and Entropy are just two different ways to measure impurity, but both usually lead to similar splits.\n",
        "\n",
        "**Q3.** What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each.\n",
        "\n",
        "- Pre-Pruning (also called Early Stopping)\n",
        " 1. The tree stops growing early.\n",
        " 2. We set some conditions before training (like maximum depth, minimum samples per leaf, or minimum information gain).\n",
        " 3.\tIf the condition is met, the tree will not split further.\n",
        "\n",
        "- Advantage:\n",
        " 1. Saves time and memory because the tree doesn’t grow too big.\n",
        " 2. Example: In real-time fraud detection, a small tree with limited depth gives faster predictions.\n",
        "\n",
        "- Post-Pruning (also called Pruning after training):\n",
        " 1. The tree is grown fully first, and then unnecessary branches are cut back.\n",
        " 2. This is done by checking which branches don’t improve accuracy much (often using a validation set).\n",
        "- Advantage:\n",
        " 1. Produces a simpler and more accurate model by removing overfitting.\n",
        " 2. Example: In medical diagnosis, post-pruning makes the model more generalizable, avoiding overly specific rules.\n",
        "\n",
        "\n",
        "**Q4.** What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?\n",
        "\n",
        " - Information Gain in Decision Trees is a measure of how much uncertainty or impurity in the data is reduced after splitting it using a particular feature. It is calculated as the difference between the entropy of the parent node and the weighted average entropy of the child nodes after the split. In simple terms, Information Gain shows how well a feature separates the data into pure groups, where each group contains mostly a single class. It is important because, at each step, a Decision Tree must decide which feature to use for splitting, and the feature with the highest Information Gain is chosen. This ensures that the tree makes the data more organized and reduces confusion, leading to better classification accuracy.\n",
        "\n",
        "**Q5.** What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "\n",
        "- Real-World Applications of Decision Trees:\n",
        " 1. Medical Diagnosis – predicting whether a patient has a disease based on symptoms.\n",
        " 2. Fraud Detection – identifying fraudulent transactions in banking or insurance.\n",
        " 3.\tCustomer Churn Prediction – checking if a customer is likely to leave a service.\n",
        " 4. Credit Scoring & Loan Approval – deciding whether to approve a loan based on applicant details.\n",
        " 5. Marketing & Sales – segmenting customers and recommending products.\n",
        " 6.\tManufacturing – quality control and fault detection in production.\n",
        " 7. Education – predicting student performance based on attendance, study habits, etc.\n",
        "\n",
        "- Advantages of Decision Trees:\n",
        " 1. Easy to understand and interpret – works like a flowchart of questions.\n",
        " 2. No need for data scaling – can handle raw data without normalization.\n",
        " 3. Handles both numerical and categorical data.\n",
        " 4.\tWorks well with small datasets.\n",
        " 5. Fast prediction time – once built, the tree is quick to use.\n",
        "- Limitations of Decision Trees:\n",
        " 1. Prone to overfitting – can grow too complex and memorize the data.\n",
        " 2. Unstable – small changes in data may create a very different tree.\n",
        " 3. Biased towards features with more levels (many unique values).\n",
        " 4. Less accurate compared to ensemble methods like Random Forests or Gradient Boosting.\n",
        " 5. Not good for continuous predictions if the tree is very shallow.\n",
        "\n",
        "Dataset Info:\n",
        "● Iris Dataset for classification tasks (sklearn.datasets.load_iris() or\n",
        "provided CSV).\n",
        "● Boston Housing Dataset for regression tasks\n",
        "(sklearn.datasets.load_boston() or provided CSV).\n",
        "\n"
      ],
      "metadata": {
        "id": "B6L-SaFR4P-j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8vcnF8v32cg",
        "outputId": "ad64e771-30a5-4060-fd67-d3c16753d491"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.0\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0167\n",
            "petal length (cm): 0.9061\n",
            "petal width (cm): 0.0772\n"
          ]
        }
      ],
      "source": [
        "#Q6. Write a Python program to:\n",
        "# ● Load the Iris Dataset\n",
        "# ● Train a Decision Tree Classifier using the Gini criterion\n",
        "# ● Print the model’s accuracy and feature importances\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Decision Tree Accuracy:\", accuracy)\n",
        "\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q7. Write a Python program to:\n",
        "# ● Load the Iris Dataset\n",
        "# ● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "# a fully-grown tree.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "tree_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "tree_limited.fit(X_train, y_train)\n",
        "y_pred_limited = tree_limited.predict(X_test)\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "\n",
        "tree_full = DecisionTreeClassifier(random_state=42)\n",
        "tree_full.fit(X_train, y_train)\n",
        "y_pred_full = tree_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "print(\"Accuracy with max_depth=3:\", accuracy_limited)\n",
        "print(\"Accuracy with full tree  :\", accuracy_full)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AtrMApB_nHb",
        "outputId": "515f234f-23d1-4c27-9bc1-010790d8bd23"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with max_depth=3: 1.0\n",
            "Accuracy with full tree  : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q8. Write a Python program to:\n",
        "# ● Load the California Housing dataset from sklearn\n",
        "# ● Train a Decision Tree Regressor\n",
        "# ● Print the Mean Squared Error (MSE) and feature importances\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(housing.feature_names, regressor.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjlZYzfuBVpf",
        "outputId": "3e41b2b4-ad8d-4ae0-dd07-41e3b5f006ea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.495235205629094\n",
            "Feature Importances:\n",
            "MedInc: 0.5285\n",
            "HouseAge: 0.0519\n",
            "AveRooms: 0.0530\n",
            "AveBedrms: 0.0287\n",
            "Population: 0.0305\n",
            "AveOccup: 0.1308\n",
            "Latitude: 0.0937\n",
            "Longitude: 0.0829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q9. Write a Python program to:\n",
        "# ● Load the Iris Dataset\n",
        "# ● Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV\n",
        "# ● Print the best parameters and the resulting model accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "param_grid = {\n",
        "    \"max_depth\": [2, 3, 4, 5, None],\n",
        "    \"min_samples_split\": [2, 3, 4, 5, 10]\n",
        "}\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid,\n",
        "                           cv=5, scoring=\"accuracy\", n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Model Accuracy on Test Set:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9n0R-dRCR9h",
        "outputId": "4623fccf-faf3-44be-de21-a137134d6420"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Model Accuracy on Test Set: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q10.** Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "* Handle the missing values\n",
        "* Encode the categorical features\n",
        "* Train a Decision Tree model\n",
        "* Tune its hyperparameters\n",
        "* Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n",
        "\n",
        " **Step-by-Step Process:**\n",
        "\n",
        "1. Handling Missing Values\n",
        "The first step would be to carefully check the dataset for missing values. If some features have only a small number of missing entries, I would use strategies like replacing them with the mean (for numerical features), median (if there are outliers), or mode (for categorical features). If a feature has too many missing values and doesn’t add much importance, I might even drop it to avoid noise in the model.\n",
        "\n",
        "2. Encoding the Categorical Features\n",
        "Since Decision Trees can only work with numbers, I need to convert categorical variables into numeric form. For categorical variables with no natural order (like “Male” or “Female”), I would use One-Hot Encoding. For features with a natural order (like “Low”, “Medium”, “High”), I could use Label Encoding. This ensures the tree can split the data properly without misunderstanding the categories.\n",
        "\n",
        "3. Training a Decision Tree Model\n",
        "Once the dataset is clean and encoded, I would split it into training and test sets, usually 80% for training and 20% for testing. Then, I’d create a Decision Tree Classifier using scikit-learn and fit it on the training data. At this stage, I would start with default settings just to get a baseline performance.\n",
        "\n",
        "4. Hyperparameter Tuning\n",
        "Decision Trees are powerful but can easily overfit, so tuning is very important. I would use GridSearchCV or RandomizedSearchCV to try different values of parameters like max_depth (how deep the tree can grow), min_samples_split (minimum samples to split a node), and min_samples_leaf (minimum samples at a leaf). The goal is to find the best balance between accuracy and generalization.\n",
        "\n",
        "5. Evaluating Performance\n",
        "To evaluate the model, I would not just look at accuracy, but also metrics like precision, recall, and F1-score, since in healthcare, predicting false negatives (saying “no disease” when the patient actually has it) can be very risky. I would also look at the confusion matrix to see how well the model is distinguishing between diseased and non-diseased patients. If needed, I might also use cross-validation for more reliable performance estimates.\n",
        "\n",
        "**Business Value in Real-World Setting**\n",
        "\n",
        "- A Decision Tree model for disease prediction could be very valuable in healthcare. It can help doctors and hospitals quickly identify high-risk patients based on their health records, reducing the time needed for manual assessments. This means patients who are more likely to have the disease can be prioritized for further medical testing or treatment, leading to earlier interventions and potentially saving lives. From a business perspective, this model can also reduce healthcare costs by focusing resources on patients who need them the most and minimizing unnecessary tests for low-risk patients. Moreover, since Decision Trees are easy to explain, doctors and healthcare providers can trust and understand the reasoning behind each prediction, making the model more practical in real-world decision-making."
      ],
      "metadata": {
        "id": "cThXqbIGC-ZF"
      }
    }
  ]
}